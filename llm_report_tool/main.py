#!/usr/bin/env python
"""
Main entry module for the LLM News Report Tool.
Provides complete workflow control and command line interface.
"""
import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Optional

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd

from llm_report_tool.processors.classifier import run as run_classifier
from llm_report_tool.processors.data_cleaner import run as run_cleaner
from llm_report_tool.processors.latex_report_generator import run as run_latex_report_generator
from llm_report_tool.processors.summarizer import run as run_summarizer
from llm_report_tool.scrapers.reddit_scraper import run as run_scraper
from llm_report_tool.utils.config import config
from llm_report_tool.utils.logging_config import get_logger, setup_logging


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="LLM News Daily Report Generator - Automatically scrape LLM-related news from Reddit and generate summary reports",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # General options
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging output"
    )
    parser.add_argument(
        "--skip-scrape", action="store_true", help="Skip scraping phase, use existing data"
    )
    parser.add_argument(
        "--skip-clean", action="store_true", help="Skip data quality analysis phase"
    )
    parser.add_argument("--skip-summary", action="store_true", help="Skip summary generation phase")
    parser.add_argument("--skip-topic", action="store_true", help="Skip topic analysis phase")

    # Output format options - PDF only
    parser.add_argument("--no-pdf", action="store_true", help="Do not generate PDF format report")

    # Input/output options
    parser.add_argument(
        "--reddit-url", type=str, help=f"Reddit subreddit URL, default: {config.reddit_url}"
    )
    parser.add_argument(
        "--hours",
        type=int,
        help=f"Filter news from how many hours ago, default: {config.post_cleanup_hours} hours (current day)",
    )
    parser.add_argument("--output-dir", type=str, help="Specify output directory")

    # New argument for classifier input
    parser.add_argument(
        "--classifier-input-file",
        type=str,
        help="Specify input summary file path for the classifier",
    )

    # New argument for report generator input
    parser.add_argument(
        "--report-input-file",
        type=str,
        help="Specify input JSON file path for the PDF report generator",
    )

    # Demo mode for testing
    parser.add_argument("--demo", action="store_true", help="Run in demo mode with test data")

    # Quick mode - skip API-dependent steps when no API key
    parser.add_argument(
        "--quick", action="store_true", help="Quick mode: skip API calls when no valid API key"
    )

    # No-API mode - use rule-based processing only
    parser.add_argument(
        "--no-api",
        action="store_true",
        help="No-API mode: use rule-based processing, skip summaries and classification",
    )

    # Resume options for interrupted workflows
    parser.add_argument(
        "--resume-from-summary",
        action="store_true",
        help="Resume from summary step: skip scraping and cleaning, use existing data files",
    )
    parser.add_argument(
        "--resume-from-report",
        action="store_true",
        help="Resume from report step: skip everything, generate report from existing classified data",
    )
    parser.add_argument(
        "--auto-resume",
        action="store_true",
        help="Automatically detect where to resume based on existing files",
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="Show status of existing files and suggest resume options",
    )

    # Performance options
    parser.add_argument("--python-version", type=str, default="3.10", help="Specify Python version")

    return parser.parse_args()


def setup_logging_config(verbose: bool = False) -> logging.Logger:
    """Configure logging level."""
    log_level = "DEBUG" if verbose else config.log_level

    logger = setup_logging(
        log_level=log_level,
        log_to_file=config.log_to_file,
        log_to_console=config.log_to_console,
        structured_logging=config.structured_logging,
    )

    if verbose:
        logger.debug("å·²å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º")

    return logger


def create_demo_data() -> bool:
    """Create demo data for testing the workflow."""
    logger = get_logger(__name__)

    try:
        # Create demo Reddit posts data
        demo_data = [
            {
                "post_title": "New DeepSeek Model Released - Performance Breakthrough",
                "post_content": "DeepSeek has just released their latest model with impressive performance on reasoning tasks. The model shows significant improvements over previous versions in mathematical reasoning and code generation. Early benchmarks suggest it's competitive with GPT-4 level performance while being more efficient.",
                "post_url": "https://www.reddit.com/r/LocalLLaMA/comments/demo1/",
                "post_date": config.current_date,
                "quality_score": 0.9,
            },
            {
                "post_title": "Open Source LLM Training: Best Practices Guide",
                "post_content": "This comprehensive guide covers the best practices for training large language models from scratch. Topics include data preparation, model architecture choices, training strategies, and evaluation metrics. The author shares lessons learned from training multiple models.",
                "post_url": "https://www.reddit.com/r/LocalLLaMA/comments/demo2/",
                "post_date": config.current_date,
                "quality_score": 0.85,
            },
            {
                "post_title": "LLM Benchmarking: New MMLU-Pro Results",
                "post_content": "Latest MMLU-Pro benchmark results show interesting trends in model performance across different domains. Claude-3.5 and GPT-4o lead in most categories, but some open source models are showing surprising strength in specific areas like mathematics and coding.",
                "post_url": "https://www.reddit.com/r/LocalLLaMA/comments/demo3/",
                "post_date": config.current_date,
                "quality_score": 0.8,
            },
        ]

        # Save to Excel file
        df = pd.DataFrame(demo_data)
        df.to_excel(config.reddit_posts_file, index=False)
        logger.info(f"æ¼”ç¤ºæ•°æ®å·²ä¿å­˜åˆ°: {config.reddit_posts_file}")

        # Also save as cleaned data
        df.to_excel(config.cleaned_posts_file, index=False)
        logger.info(f"æ¼”ç¤ºæ•°æ®å·²ä¿å­˜åˆ°: {config.cleaned_posts_file}")

        return True

    except Exception as e:
        logger.error(f"åˆ›å»ºæ¼”ç¤ºæ•°æ®æ—¶å‡ºé”™: {e}")
        return False


def check_existing_files() -> dict:
    """Check what workflow files already exist."""
    logger = get_logger(__name__)

    files_status = {
        "scraped_data": config.reddit_posts_file.exists(),
        "cleaned_data": config.cleaned_posts_file.exists(),
        "summaries": config.summaries_file.exists(),
        "classified_data": (
            config.data_dir / f"classified_summaries_{config.current_date}.json"
        ).exists(),
        "pdf_report": (
            config.reports_dir / f"{config.current_date}-{config.report_prefix}.pdf"
        ).exists(),
    }

    logger.info("Existing files status:")
    for file_type, exists in files_status.items():
        status = "âœ… EXISTS" if exists else "âŒ MISSING"
        logger.info(f"  {file_type}: {status}")

    return files_status


def determine_resume_point(files_status: dict) -> str:
    """Determine the best resume point based on existing files."""
    if files_status["classified_data"]:
        return "report"
    elif files_status["summaries"]:
        return "classification"
    elif files_status["cleaned_data"]:
        return "summary"
    elif files_status["scraped_data"]:
        return "cleaning"
    else:
        return "scraping"


def show_status_and_suggestions() -> None:
    """Show current file status and suggest resume options."""
    logger = get_logger(__name__)

    logger.info("ğŸ“Š LLM Report Tool - Current Status")
    logger.info("=" * 50)

    files_status = check_existing_files()
    resume_point = determine_resume_point(files_status)

    logger.info("")
    logger.info("ğŸ’¡ Suggestions based on current state:")

    if resume_point == "scraping":
        logger.info("  ğŸŸ¡ No data files found - run full workflow:")
        logger.info("     poetry run python main.py")
    elif resume_point == "cleaning":
        logger.info("  ğŸŸ¡ Scraped data found - resume from cleaning:")
        logger.info("     poetry run python main.py --skip-scrape")
    elif resume_point == "summary":
        logger.info("  ğŸŸ¢ Cleaned data found - resume from summary:")
        logger.info("     poetry run python main.py --resume-from-summary")
    elif resume_point == "classification":
        logger.info("  ğŸŸ¢ Summaries found - resume from classification:")
        logger.info("     poetry run python main.py --skip-scrape --skip-clean --skip-summary")
    elif resume_point == "report":
        logger.info("  ğŸ”µ Classified data found - generate report only:")
        logger.info("     poetry run python main.py --resume-from-report")

        if files_status["pdf_report"]:
            logger.info("  âœ… PDF report already exists!")

    logger.info("")
    logger.info("ğŸ”„ Auto-resume option:")
    logger.info("     poetry run python main.py --auto-resume")
    logger.info("")


def handle_resume_options(args: argparse.Namespace) -> None:
    """Handle resume options and set appropriate skip flags."""
    logger = get_logger(__name__)

    if args.auto_resume:
        files_status = check_existing_files()
        resume_point = determine_resume_point(files_status)
        logger.info(f"ğŸ”„ Auto-resume detected: starting from {resume_point}")

        if resume_point == "report":
            args.resume_from_report = True
        elif resume_point == "classification":
            args.skip_scrape = True
            args.skip_clean = True
            args.skip_summary = True
        elif resume_point == "summary":
            args.resume_from_summary = True
        elif resume_point == "cleaning":
            args.skip_scrape = True

    if args.resume_from_report:
        logger.info("ğŸ“„ Resume from report: skipping all steps except PDF generation")
        args.skip_scrape = True
        args.skip_clean = True
        args.skip_summary = True
        args.skip_topic = True

        # Check if classified data exists
        classified_file = config.data_dir / f"classified_summaries_{config.current_date}.json"
        if not classified_file.exists():
            logger.error(f"âŒ Cannot resume from report: {classified_file} not found")
            logger.error("Please run the full workflow first or use a different resume option")
            exit(1)

    elif args.resume_from_summary:
        logger.info("ğŸ“ Resume from summary: skipping scraping and cleaning")
        args.skip_scrape = True
        args.skip_clean = True

        # Check if cleaned data exists
        if not config.cleaned_posts_file.exists():
            logger.error(f"âŒ Cannot resume from summary: {config.cleaned_posts_file} not found")
            logger.error("Please run scraping and cleaning first or use --auto-resume")
            exit(1)


def update_config_from_args(args: argparse.Namespace) -> None:
    """Update configuration based on command line arguments."""
    logger = get_logger(__name__)

    if args.reddit_url:
        config.reddit_url = args.reddit_url
        logger.debug(f"å·²è®¾ç½®Reddit URLä¸º: {config.reddit_url}")

    if args.hours:
        config.post_cleanup_hours = args.hours
        logger.debug(f"å·²è®¾ç½®æ–°é—»è¿‡æ»¤æ—¶é—´ä¸º: {config.post_cleanup_hours}å°æ—¶")

    if args.output_dir:
        output_dir = Path(args.output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)

        # æ›´æ–°è¾“å‡ºè·¯å¾„
        config.reports_dir = output_dir
        logger.debug(f"å·²è®¾ç½®è¾“å‡ºç›®å½•ä¸º: {output_dir}")


def run_workflow(args: argparse.Namespace) -> bool:
    """Run the complete workflow."""
    logger = get_logger(__name__)
    success = True

    # 1. çˆ¬å–é˜¶æ®µ
    if not args.skip_scrape:
        logger.info("=== å¼€å§‹æ‰§è¡ŒRedditçˆ¬è™« ===")
        if args.demo:
            logger.info("æ¼”ç¤ºæ¨¡å¼ï¼šåˆ›å»ºç¤ºä¾‹æ•°æ®...")
            success = create_demo_data()
        else:
            success = run_scraper()

        if not success:
            logger.error("çˆ¬è™«é˜¶æ®µå¤±è´¥")
            return False
    else:
        logger.info("å·²è·³è¿‡çˆ¬è™«é˜¶æ®µ")

    # 2. æ•°æ®æ¸…æ´—é˜¶æ®µ
    if not args.skip_clean:
        logger.info("=== å¼€å§‹æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æ ===")
        if args.demo:
            logger.info("æ¼”ç¤ºæ¨¡å¼ï¼šè·³è¿‡APIè´¨é‡åˆ†æï¼Œä½¿ç”¨é¢„è®¾åˆ†æ•°")
            success = True  # Skip data cleaning in demo mode
        elif args.no_api:
            # Force rule-based scoring in data cleaner
            logger.info("æ— APIæ¨¡å¼ï¼šä½¿ç”¨åŸºäºè§„åˆ™çš„è´¨é‡è¯„åˆ†")
            from llm_report_tool.processors.data_cleaner import DataCleaner

            cleaner = DataCleaner()
            cleaner.api_available = False  # Force rule-based scoring
            df = cleaner.analyze_data()
            success = not df.empty
        else:
            success = run_cleaner()

        if not success:
            logger.error("æ•°æ®è´¨é‡åˆ†æé˜¶æ®µå¤±è´¥")
            return False
    else:
        logger.info("å·²è·³è¿‡æ•°æ®è´¨é‡åˆ†æé˜¶æ®µ")

    # 3. æ‘˜è¦ç”Ÿæˆé˜¶æ®µ
    if not args.skip_summary:
        logger.info("=== å¼€å§‹æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ ===")
        if not run_summarizer():
            logger.error("æ‘˜è¦ç”Ÿæˆé˜¶æ®µå¤±è´¥")
            return False
    else:
        logger.info("å·²è·³è¿‡æ‘˜è¦ç”Ÿæˆé˜¶æ®µ")

    # 4. æ™ºèƒ½åˆ†ç±»é˜¶æ®µ
    if not args.skip_topic:
        logger.info("=== å¼€å§‹æ‰§è¡Œæ‘˜è¦æ™ºèƒ½åˆ†ç±» ===")
        # Pass the specific input file if provided
        if not run_classifier(input_file=args.classifier_input_file):
            logger.warning("æ‘˜è¦æ™ºèƒ½åˆ†ç±»é˜¶æ®µå¤±è´¥æˆ–éƒ¨åˆ†å¤±è´¥ï¼Œä½†å°†ç»§ç»­æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ")
    else:
        logger.info("å·²è·³è¿‡æ‘˜è¦æ™ºèƒ½åˆ†ç±»é˜¶æ®µ")

    # 5. æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ
    if not args.no_pdf:
        logger.info("=== å¼€å§‹æ‰§è¡ŒPDFæŠ¥å‘Šç”Ÿæˆ ===")
        # Use the specified report input file if provided, otherwise default to today's classified file
        report_input_path = args.report_input_file or str(
            config.data_dir / f"classified_summaries_{config.current_date}.json"
        )
        logger.info(f"å°†ä½¿ç”¨ä»¥ä¸‹æ–‡ä»¶ç”ŸæˆæŠ¥å‘Š: {report_input_path}")
        if not run_latex_report_generator(classified_summary_file=report_input_path):
            logger.error("PDFæŠ¥å‘Šç”Ÿæˆé˜¶æ®µå¤±è´¥")
            success = False
    else:
        logger.info("å·²è·³è¿‡PDFæŠ¥å‘Šç”Ÿæˆé˜¶æ®µ")

    return success


def main() -> int:
    """Main function entry point."""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger = setup_logging_config(args.verbose)

    # Handle status command first
    if args.status:
        show_status_and_suggestions()
        return 0

    # æ£€æŸ¥Pythonç‰ˆæœ¬
    logger.info(f"ä½¿ç”¨Pythonç‰ˆæœ¬: {args.python_version}")

    # Handle resume options first (this may set skip flags)
    handle_resume_options(args)

    # æ›´æ–°é…ç½®
    update_config_from_args(args)

    # æ£€æŸ¥APIå¯†é’¥
    api_key_valid = (
        config.deepseek_api_key and config.deepseek_api_key != "your_deepseek_api_key_here"
    )

    if args.no_api:
        logger.info("æ— APIæ¨¡å¼ï¼šå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„å¤„ç†ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆå’Œæ™ºèƒ½åˆ†ç±»åŠŸèƒ½")
        args.skip_summary = True
        args.skip_topic = True
    elif not api_key_valid and not (args.skip_summary and args.skip_topic):
        if args.quick:
            logger.warning("å¿«é€Ÿæ¨¡å¼ï¼šæœªè®¾ç½®æœ‰æ•ˆAPIå¯†é’¥ï¼Œå°†è·³è¿‡æ‘˜è¦ç”Ÿæˆå’Œæ™ºèƒ½åˆ†ç±»åŠŸèƒ½")
            args.skip_summary = True
            args.skip_topic = True
        else:
            logger.error("é”™è¯¯ï¼šæœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œæ‘˜è¦ç”Ÿæˆå’Œæ™ºèƒ½åˆ†ç±»åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚")
            logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨--skip-summaryå’Œ--skip-topicé€‰é¡¹ï¼Œæˆ–æ·»åŠ --quickæˆ–--no-apiæ ‡å¿—è·³è¿‡ç›¸å…³åŠŸèƒ½ã€‚")
            return 1

    try:
        # è¿è¡Œå·¥ä½œæµ
        if run_workflow(args):
            logger.info(f"âœ… LLMæ–°é—»æ—¥æŠ¥ç”Ÿæˆå®Œæˆï¼")
            if not args.no_pdf:
                pdf_path = config.reports_dir / f"{config.current_date}-{config.report_prefix}.pdf"
                logger.info(f"PDFæŠ¥å‘Šå·²ä¿å­˜è‡³: {pdf_path}")
            return 0
        else:
            logger.error("âŒ LLMæ–°é—»æ—¥æŠ¥ç”Ÿæˆéƒ¨åˆ†å¤±è´¥")
            return 1
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 130
    except Exception as e:
        logger.exception(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1


# Add the execution block here
if __name__ == "__main__":
    sys.exit(main())
