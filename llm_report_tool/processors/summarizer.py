"""
Text summarization module that uses SiliconFlow API to generate summaries from cleaned data.
"""
import argparse
import json
import os
import random
import re
import time
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd
import requests

from ..exceptions import APIError
from ..utils.config import config, logger


class TextSummarizer:
    """Text summarization class that uses DeepSeek API to generate summaries."""

    def __init__(
        self,
        input_file: Optional[Union[str, Path]] = None,
        output_file: Optional[Union[str, Path]] = None,
        api_key: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–æ€»ç»“å™¨

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            api_key: DeepSeek APIå¯†é’¥ï¼Œé»˜è®¤ä»é…ç½®ä¸­è·å–
        """
        self.input_file = Path(input_file) if input_file else config.cleaned_posts_file
        self.output_file = Path(output_file) if output_file else config.summaries_file
        self.api_key = api_key or config.deepseek_api_key
        self.batch_size_min = config.summary_batch_size_min
        self.batch_size_max = config.summary_batch_size_max
        self.max_retries = 3  # åˆç†çš„é‡è¯•æ¬¡æ•°
        self.base_url = "https://api.deepseek.com"  # Official DeepSeek API endpoint

        if not self.api_key:
            raise ValueError("æœªæä¾›DeepSeek APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡DEEPSEEK_API_KEYæˆ–é€šè¿‡å‚æ•°æä¾›")

        # ä¸å†éœ€è¦ä¸“é—¨çš„APIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨æ ‡å‡†requests

        # APIä½¿ç”¨ç»Ÿè®¡
        self.request_count = 0
        self.successful_requests = 0
        self.failed_requests = 0

        # APIé…ç½® - ä½¿ç”¨å®˜æ–¹DeepSeekæ¨¡å‹
        self.model_name = "deepseek-chat"  # Official DeepSeek model
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }
        self.generation_config = {
            "temperature": config.temperature_summarizer,
            "max_tokens": 800,  # Sufficient for quality summaries while maintaining efficiency
            "top_p": 0.95,
        }

        logger.info(f"DeepSeek API å·²åˆå§‹åŒ–ï¼Œtemperature={config.temperature_summarizer}")

    def test_api_connectivity(self) -> bool:
        """
        Test API connectivity with a simple request

        Returns:
            True if API is accessible, False otherwise
        """
        try:
            logger.info("Testing API connectivity...")

            # Simple test message
            test_data = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10,
                "temperature": 0.1,
            }

            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=test_data,
                timeout=20,  # Increased timeout based on testing
            )

            if response.status_code == 200:
                logger.info("âœ… API connectivity test successful")
                return True
            elif response.status_code == 429:
                logger.warning("âš ï¸ API rate limit exceeded during connectivity test")
                return False
            elif response.status_code == 401:
                logger.error("âŒ API authentication failed - check API key")
                return False
            else:
                logger.error(
                    f"âŒ API connectivity test failed with status {response.status_code}: {response.text}"
                )
                return False

        except requests.exceptions.Timeout:
            logger.error("âŒ API connectivity test timed out")
            return False
        except requests.exceptions.ConnectionError:
            logger.error("âŒ API connectivity test failed - connection error")
            return False
        except Exception as e:
            logger.error(f"âŒ API connectivity test failed with error: {e}")
            return False

    def check_rate_limits(self) -> bool:
        """
        Check API usage statistics (DeepSeek has no hard rate limits)

        Returns:
            Always True as DeepSeek doesn't impose strict rate limits
        """
        # DeepSeek API doesn't have strict rate limits, just track usage
        if self.successful_requests > 0 and self.successful_requests % 10 == 0:
            logger.info(f"ğŸ“Š APIä½¿ç”¨ç»Ÿè®¡: æˆåŠŸ {self.successful_requests} æ¬¡ï¼Œå¤±è´¥ {self.failed_requests} æ¬¡")

        return True  # DeepSeek allows unlimited requests

    def generate_prompt(self, post: Dict) -> str:
        """
        æ ¹æ®å•ä¸ªå¸–å­ç”Ÿæˆæç¤ºè¯

        Args:
            post: å•ä¸ªå¸–å­å­—å…¸ï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹å’Œå¯é€‰çš„å›¾ç‰‡URL

        Returns:
            ç”¨äºç”Ÿæˆæ‘˜è¦çš„æç¤ºè¯
        """
        title = post.get("post_title", "æ— æ ‡é¢˜")
        content = post.get("post_content", "æ— å†…å®¹").strip()

        # é™åˆ¶å¸–å­çš„é•¿åº¦ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
        max_content_length = 1500  # å•ä¸ªå¸–å­å¯ä»¥é€‚å½“å¢åŠ é•¿åº¦
        if len(content) > max_content_length:
            content = content[:max_content_length] + "...(å†…å®¹å·²æˆªæ–­)"

        post_details = f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content}"
        prompt_template = """
        è¯·æ€»ç»“ä»¥ä¸‹Reddit LLMç›¸å…³å¸–å­ï¼Œè¦æ±‚ï¼š
        1. ä½¿ç”¨ä¸­æ–‡ï¼Œç®€æ´æ¸…æ™°åœ°æ¦‚æ‹¬æ ¸å¿ƒä¿¡æ¯
        2. ç”¨(1)(2)(3)åˆ†ç‚¹ç»„ç»‡
        3. å¿ å®åæ˜ åŸæ–‡æ ¸å¿ƒä¿¡æ¯ï¼Œä¸é—æ¼é‡è¦ç»†èŠ‚
        4. ä¿æŒæŠ€æœ¯æœ¯è¯­åŸæ ·

        å¸–å­å†…å®¹ï¼š
        {details}
        """

        return prompt_template.format(details=post_details)

    def _make_api_call_with_retry(
        self, prompt: str, log_identifier: str, max_retries: int = 3
    ) -> Optional[str]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨

        Args:
            prompt: æç¤ºè¯
            log_identifier: ç”¨äºæ—¥å¿—è®°å½•çš„æ ‡è¯†ç¬¦ (ä¾‹å¦‚ï¼šå¸–å­ç´¢å¼•æˆ–æ ‡é¢˜)

        Returns:
            APIå“åº”æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        for attempt in range(max_retries):
            try:
                # Track request attempt
                self.request_count += 1
                logger.info(
                    f"ğŸ“Š APIè¯·æ±‚è®¡æ•°: æ€»è®¡ {self.request_count} æ¬¡ (æˆåŠŸ {self.successful_requests}, å¤±è´¥ {self.failed_requests})"
                )

                if attempt > 0:
                    # Simpler backoff strategy for fewer retries
                    base_delay = 2 ** min(attempt - 1, 2)  # Cap at 4 seconds
                    jitter = random.uniform(0.8, 1.2)
                    delay = base_delay * jitter
                    logger.info(
                        f"ç¬¬ {attempt+1}/{max_retries} æ¬¡å°è¯•è°ƒç”¨APIï¼Œé’ˆå¯¹: {log_identifier}ï¼Œç­‰å¾… {delay:.1f} ç§’..."
                    )
                    time.sleep(delay)

                # æ„å»ºè¯·æ±‚æ•°æ®
                data = {
                    "model": self.model_name,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦å·¥å…·ï¼Œæ“…é•¿æ€»ç»“æŠ€æœ¯å†…å®¹ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ï¼Œæä¾›å‡†ç¡®ã€ç®€æ´çš„æ‘˜è¦ï¼Œç¡®ä¿æ€»ç»“åœ¨300-400å­—ä¹‹é—´ã€‚",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    **self.generation_config,
                }

                # è°ƒç”¨DeepSeek API
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=data,
                    timeout=30,  # DeepSeekæ”¯æŒé•¿æ—¶é—´å¤„ç†ï¼Œæœ€å¤š30åˆ†é’Ÿ
                )

                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()
                response_data = response.json()

                if "choices" in response_data and len(response_data["choices"]) > 0:
                    content = response_data["choices"][0]["message"]["content"]
                    self.successful_requests += 1
                    if attempt > 0:
                        logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡å°è¯•) é’ˆå¯¹: {log_identifier}")
                    logger.info(f"æˆåŠŸè·å¾—APIå“åº”ï¼Œé•¿åº¦ï¼š{len(content)} å­—ç¬¦")

                    # è®°å½•æ€»ç»“é•¿åº¦ä¿¡æ¯ï¼ˆä»…ç”¨äºç›‘æ§ï¼Œä¸é™åˆ¶ï¼‰
                    char_count = len(content.replace(" ", "").replace("\n", ""))
                    logger.info(f"æ€»ç»“å­—ç¬¦æ•°ï¼ˆä¸å«ç©ºæ ¼å’Œæ¢è¡Œï¼‰ï¼š{char_count}")

                    return content
                else:
                    logger.warning(f"APIè¿”å›äº†æ— æ•ˆå“åº”: {response_data}")
                    continue

            except requests.exceptions.Timeout:
                logger.warning(f"APIè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}) é’ˆå¯¹ {log_identifier}")
                if attempt == max_retries - 1:
                    logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒAPIè°ƒç”¨è¶…æ—¶ï¼Œé’ˆå¯¹: {log_identifier}")
                continue

            except requests.exceptions.ConnectionError:
                logger.warning(f"ç½‘ç»œè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}) é’ˆå¯¹ {log_identifier}")
                if attempt == max_retries - 1:
                    logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç½‘ç»œè¿æ¥å¤±è´¥ï¼Œé’ˆå¯¹: {log_identifier}")
                continue

            except requests.exceptions.HTTPError as http_err:
                status_code = http_err.response.status_code if http_err.response else "unknown"

                if status_code == 429:
                    # Rate limit exceeded - DeepSeek has no hard limits but may throttle under load
                    logger.warning(f"âš ï¸ APIè¯·æ±‚æš‚æ—¶å—é™ (429) é’ˆå¯¹ {log_identifier}")
                    logger.info("DeepSeek APIæ— ä¸¥æ ¼é€Ÿç‡é™åˆ¶ï¼Œä½†é«˜å³°æœŸå¯èƒ½æš‚æ—¶é™æµ")
                    # Continue retrying for DeepSeek as limits are dynamic
                elif status_code == 401:
                    logger.error(f"âŒ APIè®¤è¯å¤±è´¥ (401) é’ˆå¯¹ {log_identifier}: è¯·æ£€æŸ¥APIå¯†é’¥")
                    return None  # Don't retry on auth errors
                elif status_code == 403:
                    logger.error(f"âŒ APIè®¿é—®è¢«æ‹’ç» (403) é’ˆå¯¹ {log_identifier}: æƒé™ä¸è¶³")
                    return None  # Don't retry on permission errors
                else:
                    logger.error(
                        f"HTTPé”™è¯¯ {status_code} (å°è¯• {attempt + 1}/{max_retries}) é’ˆå¯¹ {log_identifier}: {http_err}"
                    )

                    # Don't retry on client errors (4xx except 429)
                    if http_err.response and 400 <= http_err.response.status_code < 500:
                        logger.error(f"å®¢æˆ·ç«¯é”™è¯¯ï¼Œåœæ­¢é‡è¯•ï¼Œé’ˆå¯¹: {log_identifier}")
                        break

                    if attempt == max_retries - 1:
                        logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒHTTPé”™è¯¯æŒç»­ï¼Œé’ˆå¯¹: {log_identifier}")
                continue

            except Exception as e:
                logger.error(
                    f"APIè°ƒç”¨å‡ºç°æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}) é’ˆå¯¹ {log_identifier}: {e}"
                )
                logger.debug(traceback.format_exc())
                if attempt == max_retries - 1:
                    logger.error(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæœªçŸ¥é”™è¯¯æŒç»­ï¼Œé’ˆå¯¹: {log_identifier}")
                continue

        self.failed_requests += 1
        logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œæ— æ³•è·å– {log_identifier} çš„å“åº”")
        return None

    def summarize_posts(self) -> bool:
        """
        è¯»å–Excelæ–‡ä»¶ï¼Œä½¿ç”¨DeepSeek APIæ‰¹é‡å¤„ç†ç”Ÿæˆæ‘˜è¦ï¼Œå¹¶ä¿å­˜ç»“æœ

        Returns:
            æ˜¯å¦æˆåŠŸç”Ÿæˆæ‘˜è¦
        """
        logger.info(f"å¼€å§‹å¤„ç†æ‘˜è¦ï¼Œä»æ–‡ä»¶ {self.input_file} è¯»å–...")

        # Test API connectivity first
        if not self.test_api_connectivity():
            logger.error("APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†æ‘˜è¦")
            return False

        try:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not self.input_file.exists():
                logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
                return False

            df = pd.read_excel(self.input_file)

            if len(df) == 0:
                logger.warning("è¾“å…¥æ–‡ä»¶ä¸åŒ…å«ä»»ä½•æ•°æ®")
                return False

            logger.info(f"è¯»å–äº† {len(df)} æ¡è®°å½•ï¼Œå¼€å§‹ç”Ÿæˆæ‘˜è¦...")

            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ["post_title", "post_content"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.error(f"è¾“å…¥æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {', '.join(missing_columns)}")
                return False

            # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œä¾¿äºå¤„ç†
            records = df.to_dict("records")

            total_posts = len(records)
            summarized_count = 0
            failed_count = 0

            with open(self.output_file, "w", encoding="utf-8") as f:
                # å…ˆå†™å…¥æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
                f.write(f"# LLM ç›¸å…³æ–°é—»æ—¥æŠ¥æ‘˜è¦ ({self.input_file.stem})\n\n")
                f.write(f"åŸºäº {total_posts} æ¡é«˜è´¨é‡ Reddit å¸–å­ç”Ÿæˆ\n\n")

                for i, record in enumerate(records):
                    post_index = i + 1
                    post_title = record.get("post_title", "æ— æ ‡é¢˜")
                    post_url = record.get("post_url", "URL_Not_Found")  # è·å– URL
                    log_identifier = f"å¸–å­ {post_index}/{total_posts} ('{post_title[:30]}...')"
                    logger.info(f"æ­£åœ¨å¤„ç†: {log_identifier}")

                    # Check rate limits before processing
                    if not self.check_rate_limits():
                        logger.error("è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œåœæ­¢å¤„ç†")
                        break

                    # ç¬¬ä¸€ä¸ªå¸–å­å‰ä¸åŠ ç©ºè¡Œï¼Œåç»­å¸–å­å‰åŠ ä¸‰ä¸ªç©ºè¡Œä»¥ç¡®ä¿ä¸¤è¡Œç©ºç™½
                    if i > 0:
                        f.write("\n\n\n")  # Write three newlines

                    try:
                        prompt = self.generate_prompt(record)
                        response_text = self._make_api_call_with_retry(prompt, log_identifier)

                        if response_text:
                            # --- Post-processing --- START
                            cleaned_response_text = response_text.strip()
                            # (ç§»é™¤æŸ¥æ‰¾é“¾æ¥è¡Œçš„é€»è¾‘ï¼Œå› ä¸ºç°åœ¨ç”±Pythonå¤„ç†)
                            # link_pattern = r'\n*\s*\[åŸæ–‡é“¾æ¥\]\(.*\]\)?'
                            # match = re.search(link_pattern, cleaned_response_text, re.IGNORECASE)
                            # summary_body = cleaned_response_text
                            # link_line = ""
                            # if match:
                            #     link_start_index = match.start()
                            #     summary_body = cleaned_response_text[:link_start_index].rstrip()
                            #     link_line = cleaned_response_text[link_start_index:].strip()

                            # ç›´æ¥æ¸…ç†æ•´ä¸ªè¿”å›æ–‡æœ¬ä¸­çš„å¤šä½™æ¢è¡Œ
                            cleaned_summary_body = re.sub(
                                r"(\n\s*){2,}", "\n", cleaned_response_text
                            )

                            # (ç§»é™¤é‡æ„é€»è¾‘)
                            # final_text = cleaned_summary_body
                            # if link_line:
                            #     final_text += "\n" + link_line
                            final_text = cleaned_summary_body  # æ¸…ç†åçš„æ–‡æœ¬å³ä¸ºæœ€ç»ˆæ‘˜è¦
                            # --- Post-processing --- END

                            # å†™å…¥æ ‡é¢˜ (åé¢ä¾ç„¶æ˜¯ä¸¤ä¸ªæ¢è¡Œ)
                            f.write(f"## {post_index}. {post_title}\n\n")
                            # å†™å…¥æ¸…ç†åçš„æ‘˜è¦å†…å®¹
                            f.write(final_text)
                            # åœ¨æ‘˜è¦åå†™å…¥é“¾æ¥ (å‰é¢ä¸€ä¸ªæ¢è¡Œ)
                            f.write(f"\n[åŸæ–‡é“¾æ¥]({post_url})")

                            f.flush()
                            logger.info(f"âœ“ æˆåŠŸç”Ÿæˆæ‘˜è¦ for {log_identifier}")
                            summarized_count += 1
                        else:
                            # API è°ƒç”¨å¤±è´¥
                            f.write(f"## {post_index}. {post_title}\n\n")
                            f.write(f"*æ‘˜è¦ç”Ÿæˆå¤±è´¥*\n\n")
                            f.write(f"\n[åŸæ–‡é“¾æ¥]({post_url})")
                            f.flush()
                            logger.error(f"æ— æ³•ç”Ÿæˆæ‘˜è¦ for {log_identifier}")
                            failed_count += 1

                    except Exception as e:
                        # å…¶ä»–é”™è¯¯
                        logger.error(f"å¤„ç† {log_identifier} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                        logger.debug(traceback.format_exc())
                        f.write(f"## {post_index}. {post_title}\n\n")
                        f.write(f"*å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè·³è¿‡æ­¤å¸–*\n\n")
                        f.write(f"\n[åŸæ–‡é“¾æ¥]({post_url})")
                        f.flush()
                        failed_count += 1
                        continue

                    # Reasonable delay between requests to be respectful to API
                    delay = random.uniform(1, 3)  # Short delay since DeepSeek has no rate limits
                    logger.info(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªè¯·æ±‚")
                    time.sleep(delay)

                # å¾ªç¯ç»“æŸå
                logger.info(
                    f"æ‘˜è¦ç”Ÿæˆå®Œæˆ: æˆåŠŸ {summarized_count} ç¯‡, å¤±è´¥ {failed_count} ç¯‡ï¼Œå…±å¤„ç† {total_posts} æ¡å¸–å­"
                )
                if summarized_count > 0:
                    return True
                else:
                    logger.error("æœªèƒ½æˆåŠŸç”Ÿæˆä»»ä½•æ‘˜è¦")
                    return False

        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False


def run(input_file: Optional[str] = None, output_file: Optional[str] = None) -> bool:
    """
    æ‰§è¡Œæ‘˜è¦ç”Ÿæˆçš„ä¸»å‡½æ•°

    Args:
        input_file: å¯é€‰çš„è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: å¯é€‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        æ˜¯å¦æˆåŠŸç”Ÿæˆæ‘˜è¦
    """
    try:
        summarizer = TextSummarizer(input_file, output_file)
        return summarizer.summarize_posts()
    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆæ¨¡å—å‡ºé”™: {e}")
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¿è¡Œæ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨")
    parser.add_argument(
        "--input",
        type=str,
        help="æŒ‡å®šè¾“å…¥çš„ Excel æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ï¼šdata/reddit_posts_2024-01-01.xlsx)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨ config.cleaned_posts_file",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="æŒ‡å®šè¾“å‡ºçš„æ‘˜è¦æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ï¼šdata/my_summaries.txt)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨ config.summaries_file",
    )

    args = parser.parse_args()

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è°ƒç”¨ run å‡½æ•°
    run(input_file=args.input, output_file=args.output)
